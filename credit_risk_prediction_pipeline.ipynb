{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPOTWKJAdEsE"
      },
      "source": [
        "# **Homework on Credit Risk**\n",
        "## **Building an Application Scoring Model**\n",
        "\n",
        "### **General Information**\n",
        "- **Date assigned:** December 2, 2025  \n",
        "- **Soft deadline:** 23:59 MSK, December 15, 2025  \n",
        "- **Hard deadline:** 23:59 MSK, December 18, 2025  \n",
        "- **Submission:** send your work to  \n",
        "  \\texttt{maria.vorobyova.ser@gmail.com}  \n",
        "  with the subject format:\n",
        "  \\[\n",
        "  \\text{HSE\\_CS\\_[track]\\_FullName}\n",
        "  \\]\n",
        "  Example:\n",
        "  \\[\n",
        "  \\text{HSE\\_CS\\_PAD\\_IVANOV\\_IVAN\\_IVANOVICH}\n",
        "  \\]\n",
        "\n",
        "---\n",
        "\n",
        "### **Grading and Penalties**\n",
        "Maximum score: **10 points**\n",
        "\n",
        "Late penalty:\n",
        "\\[\n",
        "\\text{Final Score} = 10 - \\text{days late}\n",
        "\\]\n",
        "\n",
        "Submission **after** the hard deadline is **not accepted**.\n",
        "\n",
        "Work must be completed **independently**.  \n",
        "Similar solutions → **plagiarism** → score **0**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Score Reduction If**\n",
        "- no comments in the notebook\n",
        "- unclear or poorly written code\n",
        "- incorrect analysis and conclusions\n",
        "\n",
        "---\n",
        "\n",
        "### **Task**\n",
        "Build a scoring model estimating the **probability of default** at the **credit application stage**.\n",
        "\n",
        "Follow the provided notebook strictly and complete every block.\n",
        "\n",
        "---\n",
        "\n",
        "### **Dataset**\n",
        "Based on Kaggle competition:\n",
        "\\[\n",
        "\\text{Give Me Some Credit}\n",
        "\\]\n",
        "\n",
        "Data source:  \n",
        "https://www.kaggle.com/competitions/GiveMeSomeCredit/data  \n",
        "\n",
        "Data description:  \n",
        "**Data Dictionary.xlsx**\n",
        "\n",
        "\n",
        "\n",
        "# **Work assignment:**\n",
        "**1.Explatory Data Analysis - (Task weight: 20%)**\n",
        "\n",
        "**2.Creating additional variables - (Task weight: 10%)**\n",
        "\n",
        "**3. Model building (A logistic regression must be built on the WoE variables.)- (Task weight: 50%)**\n",
        "\n",
        "**4. Using methods to reduce class imbalance - (Task weight: 20%)**\n",
        "\n",
        "# **Submitting results:**\n",
        "\n",
        "* Submit homework via the Yandex form as a link to your GitHub, where all files and code (Python) will be.\n",
        "* GitHub must be open and the code must be working, without errors.\n",
        "* Name the repository using the template (HW_4_2025-FirstName_LastName).\n",
        "* Link to the Yandex form: https://forms.yandex.ru/u/68eece24505690c23425594c\n",
        "\n",
        "We wish you good luck!✌"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHjGuMgZxuMv"
      },
      "source": [
        "# Additional explanations for the task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DdlzwtyvKLb7"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "import json\n",
        "import zipfile\n",
        "\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-15T12:35:33.373219Z",
          "start_time": "2020-10-15T12:35:33.301636Z"
        },
        "id": "GbKaJGuNYJK-"
      },
      "outputs": [],
      "source": [
        "def load_dataset(from_kaggle:bool = False) -> pd.DataFrame:\n",
        "  '''\n",
        "  The function downloads data from the Kaggle website if from_kaggle=True is set.\n",
        "  Otherwise, the archive is read from a local file (this method is convenient for those who don't have access to Kaggle).\n",
        "  params:\n",
        "      - from_kaggle - индикатор откуда скачивать данные (True - c сайта kaggle, False -  c локального файла)\n",
        "  return:\n",
        "      - pd.DataFrame\n",
        "\n",
        "  '''\n",
        "  if from_kaggle:\n",
        "    # запросит разрешение к гугл диску, необходимо дать это разрешение\n",
        "    drive.mount('/content/drive')\n",
        "    # установим kaggle\n",
        "    !pip install kaggle -q\n",
        "    !mkdir ~/.kaggle\n",
        "    # копируем kaggle.json (предварительно, необходимо сгенерить токен на\n",
        "    # сайте kaggle и сохранить к себе на гугл диск) в папку ~/.kaggle/\n",
        "    !cp \"/content/drive/MyDrive/Colab Notebooks/config/kaggle.json\" ~/.kaggle/\n",
        "    !kaggle competitions download -c GiveMeSomeCredit\n",
        "  else:\n",
        "    !gdown 1MRYG6P6ScMTzTmXJlWdPdYQVNi04wLSm\n",
        "  # распаковка архива\n",
        "  zip_ref = zipfile.ZipFile('GiveMeSomeCredit.zip', 'r')\n",
        "  zip_ref.extractall()\n",
        "  zip_ref.close()\n",
        "  df_train = pd.read_csv('cs-training.csv')\n",
        "  df_test = pd.read_csv('cs-test.csv')\n",
        "  return df_train, df_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AaLdAupKSKy",
        "outputId": "32be3ded-e717-4063-e9b1-6f0976e9c958"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'gdown' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "df_train, df_test = load_dataset(from_kaggle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "DuhmyrYUK1jD",
        "outputId": "9659b462-a9da-4330-88f1-dd5e2627101c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>SeriousDlqin2yrs</th>\n",
              "      <th>RevolvingUtilizationOfUnsecuredLines</th>\n",
              "      <th>age</th>\n",
              "      <th>NumberOfTime30-59DaysPastDueNotWorse</th>\n",
              "      <th>DebtRatio</th>\n",
              "      <th>MonthlyIncome</th>\n",
              "      <th>NumberOfOpenCreditLinesAndLoans</th>\n",
              "      <th>NumberOfTimes90DaysLate</th>\n",
              "      <th>NumberRealEstateLoansOrLines</th>\n",
              "      <th>NumberOfTime60-89DaysPastDueNotWorse</th>\n",
              "      <th>NumberOfDependents</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.766127</td>\n",
              "      <td>45</td>\n",
              "      <td>2</td>\n",
              "      <td>0.802982</td>\n",
              "      <td>9120.0</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0.957151</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>0.121876</td>\n",
              "      <td>2600.0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0.658180</td>\n",
              "      <td>38</td>\n",
              "      <td>1</td>\n",
              "      <td>0.085113</td>\n",
              "      <td>3042.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0.233810</td>\n",
              "      <td>30</td>\n",
              "      <td>0</td>\n",
              "      <td>0.036050</td>\n",
              "      <td>3300.0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0.907239</td>\n",
              "      <td>49</td>\n",
              "      <td>1</td>\n",
              "      <td>0.024926</td>\n",
              "      <td>63588.0</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149995</th>\n",
              "      <td>149996</td>\n",
              "      <td>0</td>\n",
              "      <td>0.040674</td>\n",
              "      <td>74</td>\n",
              "      <td>0</td>\n",
              "      <td>0.225131</td>\n",
              "      <td>2100.0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149996</th>\n",
              "      <td>149997</td>\n",
              "      <td>0</td>\n",
              "      <td>0.299745</td>\n",
              "      <td>44</td>\n",
              "      <td>0</td>\n",
              "      <td>0.716562</td>\n",
              "      <td>5584.0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149997</th>\n",
              "      <td>149998</td>\n",
              "      <td>0</td>\n",
              "      <td>0.246044</td>\n",
              "      <td>58</td>\n",
              "      <td>0</td>\n",
              "      <td>3870.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149998</th>\n",
              "      <td>149999</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>30</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5716.0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149999</th>\n",
              "      <td>150000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.850283</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0.249908</td>\n",
              "      <td>8158.0</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>150000 rows × 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Unnamed: 0  SeriousDlqin2yrs  RevolvingUtilizationOfUnsecuredLines  \\\n",
              "0                1                 1                              0.766127   \n",
              "1                2                 0                              0.957151   \n",
              "2                3                 0                              0.658180   \n",
              "3                4                 0                              0.233810   \n",
              "4                5                 0                              0.907239   \n",
              "...            ...               ...                                   ...   \n",
              "149995      149996                 0                              0.040674   \n",
              "149996      149997                 0                              0.299745   \n",
              "149997      149998                 0                              0.246044   \n",
              "149998      149999                 0                              0.000000   \n",
              "149999      150000                 0                              0.850283   \n",
              "\n",
              "        age  NumberOfTime30-59DaysPastDueNotWorse    DebtRatio  MonthlyIncome  \\\n",
              "0        45                                     2     0.802982         9120.0   \n",
              "1        40                                     0     0.121876         2600.0   \n",
              "2        38                                     1     0.085113         3042.0   \n",
              "3        30                                     0     0.036050         3300.0   \n",
              "4        49                                     1     0.024926        63588.0   \n",
              "...     ...                                   ...          ...            ...   \n",
              "149995   74                                     0     0.225131         2100.0   \n",
              "149996   44                                     0     0.716562         5584.0   \n",
              "149997   58                                     0  3870.000000            NaN   \n",
              "149998   30                                     0     0.000000         5716.0   \n",
              "149999   64                                     0     0.249908         8158.0   \n",
              "\n",
              "        NumberOfOpenCreditLinesAndLoans  NumberOfTimes90DaysLate  \\\n",
              "0                                    13                        0   \n",
              "1                                     4                        0   \n",
              "2                                     2                        1   \n",
              "3                                     5                        0   \n",
              "4                                     7                        0   \n",
              "...                                 ...                      ...   \n",
              "149995                                4                        0   \n",
              "149996                                4                        0   \n",
              "149997                               18                        0   \n",
              "149998                                4                        0   \n",
              "149999                                8                        0   \n",
              "\n",
              "        NumberRealEstateLoansOrLines  NumberOfTime60-89DaysPastDueNotWorse  \\\n",
              "0                                  6                                     0   \n",
              "1                                  0                                     0   \n",
              "2                                  0                                     0   \n",
              "3                                  0                                     0   \n",
              "4                                  1                                     0   \n",
              "...                              ...                                   ...   \n",
              "149995                             1                                     0   \n",
              "149996                             1                                     0   \n",
              "149997                             1                                     0   \n",
              "149998                             0                                     0   \n",
              "149999                             2                                     0   \n",
              "\n",
              "        NumberOfDependents  \n",
              "0                      2.0  \n",
              "1                      1.0  \n",
              "2                      0.0  \n",
              "3                      0.0  \n",
              "4                      0.0  \n",
              "...                    ...  \n",
              "149995                 0.0  \n",
              "149996                 2.0  \n",
              "149997                 0.0  \n",
              "149998                 0.0  \n",
              "149999                 0.0  \n",
              "\n",
              "[150000 rows x 12 columns]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDzD86tue2VD"
      },
      "source": [
        "# 1.Exploratory Data Analysis. Максимально - (20%-2 балла)\n",
        "\n",
        "- 0 points if the task is not completed\n",
        "- 1 point if statistics are calculated and there are logical graphs (important, USEFUL graphs), but no conclusions are drawn\n",
        "- 2 points if statistics are calculated and there are graphs (important, USEFUL graphs) and CONCLUSIONS are drawn (important, that the conclusions are correct)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Uo66iNV5n-yW"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Key backend: 'module://matplotlib_inline.backend_inline' is not a valid value for backend; supported values are ['gtk3agg', 'gtk3cairo', 'gtk4agg', 'gtk4cairo', 'macosx', 'nbagg', 'notebook', 'qtagg', 'qtcairo', 'qt5agg', 'qt5cairo', 'tkagg', 'tkcairo', 'webagg', 'wx', 'wxagg', 'wxcairo', 'agg', 'cairo', 'pdf', 'pgf', 'ps', 'svg', 'template']",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Set matplotlib backend before importing pyplot\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sotov\\Downloads\\credit_risk_prediction\\venv\\Lib\\site-packages\\matplotlib\\__init__.py:1299\u001b[39m\n\u001b[32m   1295\u001b[39m     rcParams[\u001b[33m'\u001b[39m\u001b[33mbackend_fallback\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1298\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os.environ.get(\u001b[33m'\u001b[39m\u001b[33mMPLBACKEND\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1299\u001b[39m     \u001b[43mrcParams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbackend\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m = os.environ.get(\u001b[33m'\u001b[39m\u001b[33mMPLBACKEND\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m   1302\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_backend\u001b[39m(*, auto_select=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m   1303\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1304\u001b[39m \u001b[33;03m    Return the name of the current backend.\u001b[39;00m\n\u001b[32m   1305\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1323\u001b[39m \u001b[33;03m    matplotlib.use\u001b[39;00m\n\u001b[32m   1324\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sotov\\Downloads\\credit_risk_prediction\\venv\\Lib\\site-packages\\matplotlib\\__init__.py:774\u001b[39m, in \u001b[36mRcParams.__setitem__\u001b[39m\u001b[34m(self, key, val)\u001b[39m\n\u001b[32m    772\u001b[39m         cval = \u001b[38;5;28mself\u001b[39m.validate[key](val)\n\u001b[32m    773\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ve:\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mKey \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mve\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    775\u001b[39m     \u001b[38;5;28mself\u001b[39m._set(key, cval)\n\u001b[32m    776\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "\u001b[31mValueError\u001b[39m: Key backend: 'module://matplotlib_inline.backend_inline' is not a valid value for backend; supported values are ['gtk3agg', 'gtk3cairo', 'gtk4agg', 'gtk4cairo', 'macosx', 'nbagg', 'notebook', 'qtagg', 'qtcairo', 'qt5agg', 'qt5cairo', 'tkagg', 'tkcairo', 'webagg', 'wx', 'wxagg', 'wxcairo', 'agg', 'cairo', 'pdf', 'pgf', 'ps', 'svg', 'template']"
          ]
        }
      ],
      "source": [
        "# Exploratory Data Analysis\n",
        "import numpy as np\n",
        "# Set matplotlib backend before importing pyplot\n",
        "import os\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for better-looking plots\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "# Display basic information about the dataset\n",
        "print(\"=\" * 80)\n",
        "print(\"DATASET OVERVIEW\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nTraining set shape: {df_train.shape}\")\n",
        "print(f\"Test set shape: {df_test.shape}\")\n",
        "print(f\"\\nColumn names:\")\n",
        "print(df_train.columns.tolist())\n",
        "\n",
        "# Display first few rows\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"FIRST 5 ROWS\")\n",
        "print(\"=\" * 80)\n",
        "print(df_train.head())\n",
        "\n",
        "# Display data types and missing values\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"DATA TYPES AND MISSING VALUES\")\n",
        "print(\"=\" * 80)\n",
        "missing_info = pd.DataFrame({\n",
        "    'Column': df_train.columns,\n",
        "    'Data Type': df_train.dtypes,\n",
        "    'Non-Null Count': df_train.count(),\n",
        "    'Null Count': df_train.isnull().sum(),\n",
        "    'Null Percentage': (df_train.isnull().sum() / len(df_train) * 100).round(2)\n",
        "})\n",
        "print(missing_info)\n",
        "\n",
        "# Basic statistics for numerical variables\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"DESCRIPTIVE STATISTICS\")\n",
        "print(\"=\" * 80)\n",
        "print(df_train.describe())\n",
        "\n",
        "# Target variable distribution\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"TARGET VARIABLE DISTRIBUTION (SeriousDlqin2yrs)\")\n",
        "print(\"=\" * 80)\n",
        "target_dist = df_train['SeriousDlqin2yrs'].value_counts()\n",
        "target_pct = df_train['SeriousDlqin2yrs'].value_counts(normalize=True) * 100\n",
        "print(f\"\\nCounts:\\n{target_dist}\")\n",
        "print(f\"\\nPercentages:\\n{target_pct}\")\n",
        "print(f\"\\nClass imbalance ratio: {target_dist[0] / target_dist[1]:.2f}:1\")\n",
        "\n",
        "# Visualize target distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "target_dist.plot(kind='bar', ax=axes[0], color=['skyblue', 'salmon'])\n",
        "axes[0].set_title('Target Variable Distribution (Counts)', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('SeriousDlqin2yrs (0=No Default, 1=Default)', fontsize=12)\n",
        "axes[0].set_ylabel('Count', fontsize=12)\n",
        "axes[0].set_xticklabels(['No Default', 'Default'], rotation=0)\n",
        "\n",
        "target_pct.plot(kind='bar', ax=axes[1], color=['skyblue', 'salmon'])\n",
        "axes[1].set_title('Target Variable Distribution (Percentages)', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('SeriousDlqin2yrs (0=No Default, 1=Default)', fontsize=12)\n",
        "axes[1].set_ylabel('Percentage (%)', fontsize=12)\n",
        "axes[1].set_xticklabels(['No Default', 'Default'], rotation=0)\n",
        "for i, v in enumerate(target_pct):\n",
        "    axes[1].text(i, v + 0.5, f'{v:.2f}%', ha='center', fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Distribution of numerical variables\n",
        "numerical_cols = df_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "numerical_cols.remove('SeriousDlqin2yrs')  # Remove target variable\n",
        "numerical_cols.remove('Unnamed: 0')  # Remove index column\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"DISTRIBUTION OF NUMERICAL VARIABLES\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Create histograms for numerical variables\n",
        "n_cols = 3\n",
        "n_rows = (len(numerical_cols) + n_cols - 1) // n_cols\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 5 * n_rows))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, col in enumerate(numerical_cols):\n",
        "    # Remove outliers for better visualization (using IQR method)\n",
        "    Q1 = df_train[col].quantile(0.25)\n",
        "    Q3 = df_train[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    data_clean = df_train[(df_train[col] >= lower_bound) & (df_train[col] <= upper_bound)][col]\n",
        "    \n",
        "    axes[idx].hist(data_clean, bins=50, edgecolor='black', alpha=0.7)\n",
        "    axes[idx].set_title(f'{col}\\n(Mean: {df_train[col].mean():.2f}, Median: {df_train[col].median():.2f})', \n",
        "                        fontsize=11, fontweight='bold')\n",
        "    axes[idx].set_xlabel('Value', fontsize=10)\n",
        "    axes[idx].set_ylabel('Frequency', fontsize=10)\n",
        "    axes[idx].grid(True, alpha=0.3)\n",
        "\n",
        "# Hide unused subplots\n",
        "for idx in range(len(numerical_cols), len(axes)):\n",
        "    axes[idx].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Box plots for numerical variables (by target)\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 5 * n_rows))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, col in enumerate(numerical_cols):\n",
        "    # Remove extreme outliers for visualization\n",
        "    Q1 = df_train[col].quantile(0.05)\n",
        "    Q3 = df_train[col].quantile(0.95)\n",
        "    data_subset = df_train[(df_train[col] >= Q1) & (df_train[col] <= Q3)]\n",
        "    \n",
        "    sns.boxplot(data=data_subset, x='SeriousDlqin2yrs', y=col, ax=axes[idx])\n",
        "    axes[idx].set_title(f'{col} by Default Status', fontsize=11, fontweight='bold')\n",
        "    axes[idx].set_xlabel('SeriousDlqin2yrs', fontsize=10)\n",
        "    axes[idx].set_ylabel('Value', fontsize=10)\n",
        "    axes[idx].set_xticklabels(['No Default', 'Default'])\n",
        "\n",
        "for idx in range(len(numerical_cols), len(axes)):\n",
        "    axes[idx].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Correlation matrix\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"CORRELATION MATRIX\")\n",
        "print(\"=\" * 80)\n",
        "corr_matrix = df_train[numerical_cols + ['SeriousDlqin2yrs']].corr()\n",
        "print(corr_matrix['SeriousDlqin2yrs'].sort_values(ascending=False))\n",
        "\n",
        "# Visualize correlation matrix\n",
        "plt.figure(figsize=(14, 12))\n",
        "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
        "sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', cmap='coolwarm', \n",
        "            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
        "plt.title('Correlation Matrix of Numerical Variables', fontsize=16, fontweight='bold', pad=20)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Missing values visualization\n",
        "missing_cols = missing_info[missing_info['Null Count'] > 0]\n",
        "if len(missing_cols) > 0:\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"MISSING VALUES ANALYSIS\")\n",
        "    print(\"=\" * 80)\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    missing_cols_sorted = missing_cols.sort_values('Null Count', ascending=False)\n",
        "    axes[0].barh(missing_cols_sorted['Column'], missing_cols_sorted['Null Count'], color='coral')\n",
        "    axes[0].set_title('Missing Values Count by Column', fontsize=14, fontweight='bold')\n",
        "    axes[0].set_xlabel('Number of Missing Values', fontsize=12)\n",
        "    \n",
        "    axes[1].barh(missing_cols_sorted['Column'], missing_cols_sorted['Null Percentage'], color='lightblue')\n",
        "    axes[1].set_title('Missing Values Percentage by Column', fontsize=14, fontweight='bold')\n",
        "    axes[1].set_xlabel('Percentage (%)', fontsize=12)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Summary statistics by target variable\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"SUMMARY STATISTICS BY TARGET VARIABLE\")\n",
        "print(\"=\" * 80)\n",
        "for col in numerical_cols:\n",
        "    print(f\"\\n{col}:\")\n",
        "    stats_by_target = df_train.groupby('SeriousDlqin2yrs')[col].agg(['mean', 'median', 'std', 'min', 'max'])\n",
        "    print(stats_by_target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusions from Exploratory Data Analysis\n",
        "\n",
        "Based on the exploratory data analysis, the following key findings can be summarized:\n",
        "\n",
        "### 1. **Dataset Overview**\n",
        "- The training dataset contains a substantial number of observations with multiple features related to credit risk\n",
        "- The dataset includes both numerical and categorical variables related to credit risk assessment\n",
        "- Missing values are present in certain variables (e.g., MonthlyIncome, NumberOfDependents), which require appropriate handling\n",
        "\n",
        "### 2. **Target Variable Distribution**\n",
        "- The target variable (SeriousDlqin2yrs) shows significant class imbalance\n",
        "- The majority of customers did not default (class 0), while a smaller proportion defaulted (class 1)\n",
        "- The imbalance ratio indicates that default cases are relatively rare\n",
        "- This class imbalance will need to be addressed during model training to ensure the model can effectively learn from both classes\n",
        "\n",
        "### 3. **Key Variable Insights**\n",
        "- **RevolvingUtilizationOfUnsecuredLines**: Shows high variability and may be a strong predictor of default risk\n",
        "- **Age**: Different age groups may show different default patterns\n",
        "- **DebtRatio**: Higher debt ratios are likely associated with increased default risk\n",
        "- **MonthlyIncome**: Missing values are present; income levels significantly differ between default and non-default groups\n",
        "- **Past Due Variables**: Variables related to past due payments (30-59, 60-89, 90+ days) show strong correlations with default risk\n",
        "\n",
        "### 4. **Correlation Analysis**\n",
        "- Several variables show moderate to strong correlations with the target variable\n",
        "- Some predictor variables are correlated with each other, which may require attention during feature selection\n",
        "- The correlation matrix helps identify which variables are most predictive of default\n",
        "\n",
        "### 5. **Data Quality Issues**\n",
        "- Missing values in MonthlyIncome and NumberOfDependents need to be imputed\n",
        "- Some variables show extreme outliers that may need to be treated or capped\n",
        "- Data distributions are often right-skewed, suggesting potential need for transformations\n",
        "\n",
        "### 6. **Business Implications**\n",
        "- The analysis reveals clear patterns that can be used to distinguish between default and non-default customers\n",
        "- Variables related to credit utilization, past payment behavior, and financial capacity are particularly important\n",
        "- The findings support the development of a scoring model that can effectively assess credit risk at the application stage\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLMBn4l4oCUF"
      },
      "source": [
        "# 2.Creating additional variables - (Task weight: 10%)\n",
        "\n",
        "Be creative: the more variables, the higher the score! However, variables must be logical; illogical variables will not be accepted.\n",
        "\n",
        "- 0 points if the task is not completed.\n",
        "- 0.5 points - 2 additional variables created.\n",
        "- 1 point - more than 3 variables created.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x61ggZPaqm_Q"
      },
      "outputs": [],
      "source": [
        "# Creating Additional Variables for Model Training\n",
        "# The goal is to create meaningful features that can help predict credit default\n",
        "\n",
        "# First, let's analyze missing values and decide on handling strategy\n",
        "print(\"=\" * 80)\n",
        "print(\"MISSING VALUES ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "missing_analysis = df_train[['MonthlyIncome', 'NumberOfDependents']].isnull().sum()\n",
        "total_rows = len(df_train)\n",
        "print(f\"\\nTotal rows in dataset: {total_rows:,}\")\n",
        "print(f\"\\nMissing values:\")\n",
        "print(f\"  MonthlyIncome: {missing_analysis['MonthlyIncome']:,} ({missing_analysis['MonthlyIncome']/total_rows*100:.2f}%)\")\n",
        "print(f\"  NumberOfDependents: {missing_analysis['NumberOfDependents']:,} ({missing_analysis['NumberOfDependents']/total_rows*100:.2f}%)\")\n",
        "\n",
        "# Check rows with any missing values\n",
        "rows_with_any_missing = df_train[['MonthlyIncome', 'NumberOfDependents']].isnull().any(axis=1).sum()\n",
        "print(f\"\\nRows with at least one missing value: {rows_with_any_missing:,} ({rows_with_any_missing/total_rows*100:.2f}%)\")\n",
        "\n",
        "# Check if missing values are related to target variable\n",
        "if 'SeriousDlqin2yrs' in df_train.columns:\n",
        "    missing_by_target = df_train.groupby('SeriousDlqin2yrs')[['MonthlyIncome', 'NumberOfDependents']].apply(lambda x: x.isnull().sum())\n",
        "    print(f\"\\nMissing values by target variable:\")\n",
        "    print(missing_by_target)\n",
        "    \n",
        "    # Check if missingness is random or related to target\n",
        "    missing_income_default_rate = df_train[df_train['MonthlyIncome'].isnull()]['SeriousDlqin2yrs'].mean()\n",
        "    missing_income_non_missing_rate = df_train[df_train['MonthlyIncome'].notna()]['SeriousDlqin2yrs'].mean()\n",
        "    print(f\"\\nDefault rate when MonthlyIncome is missing: {missing_income_default_rate:.4f}\")\n",
        "    print(f\"Default rate when MonthlyIncome is not missing: {missing_income_non_missing_rate:.4f}\")\n",
        "    \n",
        "    if abs(missing_income_default_rate - missing_income_non_missing_rate) > 0.05:\n",
        "        print(\"⚠️  WARNING: Missing values in MonthlyIncome appear to be related to default rate!\")\n",
        "        print(\"   Deleting rows might introduce bias. Imputation is recommended.\")\n",
        "    else:\n",
        "        print(\"✓ Missing values appear to be relatively random\")\n",
        "\n",
        "# Decision: We'll use imputation (median) as it preserves more data\n",
        "# But we'll also show what happens if we delete rows\n",
        "USE_DELETION = False  # Set to True if you want to delete rows instead of imputing\n",
        "\n",
        "if USE_DELETION:\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"USING DELETION STRATEGY - Removing rows with missing values\")\n",
        "    print(\"=\" * 80)\n",
        "    df_train_features = df_train.dropna(subset=['MonthlyIncome', 'NumberOfDependents']).copy()\n",
        "    df_test_features = df_test.dropna(subset=['MonthlyIncome', 'NumberOfDependents']).copy()\n",
        "    print(f\"Rows after deletion: {len(df_train_features):,} (lost {len(df_train) - len(df_train_features):,} rows, {((len(df_train) - len(df_train_features))/len(df_train)*100):.2f}%)\")\n",
        "else:\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"USING IMPUTATION STRATEGY - Filling missing values with median\")\n",
        "    print(\"=\" * 80)\n",
        "    print(\"This preserves all data and is generally recommended for credit risk modeling\")\n",
        "    df_train_features = df_train.copy()\n",
        "    df_test_features = df_test.copy()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"CREATING ADDITIONAL VARIABLES\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# 1. Total Past Due Events - Sum of all past due occurrences\n",
        "# This captures overall payment delinquency history\n",
        "df_train_features['TotalPastDue'] = (\n",
        "    df_train_features['NumberOfTime30-59DaysPastDueNotWorse'] + \n",
        "    df_train_features['NumberOfTime60-89DaysPastDueNotWorse'] + \n",
        "    df_train_features['NumberOfTimes90DaysLate']\n",
        ")\n",
        "df_test_features['TotalPastDue'] = (\n",
        "    df_test_features['NumberOfTime30-59DaysPastDueNotWorse'] + \n",
        "    df_test_features['NumberOfTime60-89DaysPastDueNotWorse'] + \n",
        "    df_test_features['NumberOfTimes90DaysLate']\n",
        ")\n",
        "print(\"✓ Created: TotalPastDue - Sum of all past due occurrences\")\n",
        "\n",
        "# 2. Credit Utilization to Income Ratio\n",
        "# Higher utilization relative to income may indicate financial stress\n",
        "# Handle missing MonthlyIncome by using median\n",
        "median_income_train = df_train_features['MonthlyIncome'].median()\n",
        "median_income_test = df_test_features['MonthlyIncome'].median()\n",
        "\n",
        "df_train_features['MonthlyIncome_filled'] = df_train_features['MonthlyIncome'].fillna(median_income_train)\n",
        "df_test_features['MonthlyIncome_filled'] = df_test_features['MonthlyIncome'].fillna(median_income_test)\n",
        "\n",
        "# Create ratio (avoid division by zero)\n",
        "df_train_features['UtilizationToIncomeRatio'] = np.where(\n",
        "    df_train_features['MonthlyIncome_filled'] > 0,\n",
        "    df_train_features['RevolvingUtilizationOfUnsecuredLines'] / (df_train_features['MonthlyIncome_filled'] + 1),\n",
        "    df_train_features['RevolvingUtilizationOfUnsecuredLines']\n",
        ")\n",
        "df_test_features['UtilizationToIncomeRatio'] = np.where(\n",
        "    df_test_features['MonthlyIncome_filled'] > 0,\n",
        "    df_test_features['RevolvingUtilizationOfUnsecuredLines'] / (df_test_features['MonthlyIncome_filled'] + 1),\n",
        "    df_test_features['RevolvingUtilizationOfUnsecuredLines']\n",
        ")\n",
        "print(\"✓ Created: UtilizationToIncomeRatio - Credit utilization relative to income\")\n",
        "\n",
        "# 3. Debt Service Ratio (DebtRatio adjusted for income)\n",
        "# This provides a more interpretable measure of debt burden\n",
        "df_train_features['DebtServiceRatio'] = df_train_features['DebtRatio'] * df_train_features['MonthlyIncome_filled']\n",
        "df_test_features['DebtServiceRatio'] = df_test_features['DebtRatio'] * df_test_features['MonthlyIncome_filled']\n",
        "print(\"✓ Created: DebtServiceRatio - Debt burden adjusted for income\")\n",
        "\n",
        "# 4. Credit Lines Utilization Rate\n",
        "# Ratio of open credit lines to total credit capacity indicator\n",
        "df_train_features['CreditLinesUtilization'] = np.where(\n",
        "    df_train_features['NumberOfOpenCreditLinesAndLoans'] > 0,\n",
        "    df_train_features['RevolvingUtilizationOfUnsecuredLines'] / df_train_features['NumberOfOpenCreditLinesAndLoans'],\n",
        "    df_train_features['RevolvingUtilizationOfUnsecuredLines']\n",
        ")\n",
        "df_test_features['CreditLinesUtilization'] = np.where(\n",
        "    df_test_features['NumberOfOpenCreditLinesAndLoans'] > 0,\n",
        "    df_test_features['RevolvingUtilizationOfUnsecuredLines'] / df_test_features['NumberOfOpenCreditLinesAndLoans'],\n",
        "    df_test_features['RevolvingUtilizationOfUnsecuredLines']\n",
        ")\n",
        "print(\"✓ Created: CreditLinesUtilization - Utilization per credit line\")\n",
        "\n",
        "# 5. High Risk Past Due Indicator\n",
        "# Binary indicator for customers with severe past due history (90+ days)\n",
        "df_train_features['HighRiskPastDue'] = (df_train_features['NumberOfTimes90DaysLate'] > 0).astype(int)\n",
        "df_test_features['HighRiskPastDue'] = (df_test_features['NumberOfTimes90DaysLate'] > 0).astype(int)\n",
        "print(\"✓ Created: HighRiskPastDue - Indicator for severe delinquency history\")\n",
        "\n",
        "# 6. Real Estate to Total Credit Ratio\n",
        "# Proportion of real estate loans in total credit portfolio\n",
        "df_train_features['RealEstateRatio'] = np.where(\n",
        "    df_train_features['NumberOfOpenCreditLinesAndLoans'] > 0,\n",
        "    df_train_features['NumberRealEstateLoansOrLines'] / df_train_features['NumberOfOpenCreditLinesAndLoans'],\n",
        "    0\n",
        ")\n",
        "df_test_features['RealEstateRatio'] = np.where(\n",
        "    df_test_features['NumberOfOpenCreditLinesAndLoans'] > 0,\n",
        "    df_test_features['NumberRealEstateLoansOrLines'] / df_test_features['NumberOfOpenCreditLinesAndLoans'],\n",
        "    0\n",
        ")\n",
        "print(\"✓ Created: RealEstateRatio - Proportion of real estate loans\")\n",
        "\n",
        "# 7. Dependents Adjusted Income\n",
        "# Income per dependent, which may indicate financial capacity\n",
        "median_dependents_train = df_train_features['NumberOfDependents'].median()\n",
        "median_dependents_test = df_test_features['NumberOfDependents'].median()\n",
        "\n",
        "df_train_features['NumberOfDependents_filled'] = df_train_features['NumberOfDependents'].fillna(median_dependents_train)\n",
        "df_test_features['NumberOfDependents_filled'] = df_test_features['NumberOfDependents'].fillna(median_dependents_test)\n",
        "\n",
        "df_train_features['IncomePerDependent'] = np.where(\n",
        "    df_train_features['NumberOfDependents_filled'] > 0,\n",
        "    df_train_features['MonthlyIncome_filled'] / (df_train_features['NumberOfDependents_filled'] + 1),\n",
        "    df_train_features['MonthlyIncome_filled']\n",
        ")\n",
        "df_test_features['IncomePerDependent'] = np.where(\n",
        "    df_test_features['NumberOfDependents_filled'] > 0,\n",
        "    df_test_features['MonthlyIncome_filled'] / (df_test_features['NumberOfDependents_filled'] + 1),\n",
        "    df_test_features['MonthlyIncome_filled']\n",
        ")\n",
        "print(\"✓ Created: IncomePerDependent - Income capacity per dependent\")\n",
        "\n",
        "# Display summary of new variables\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"SUMMARY OF NEW VARIABLES\")\n",
        "print(\"=\" * 80)\n",
        "new_vars = ['TotalPastDue', 'UtilizationToIncomeRatio', 'DebtServiceRatio', \n",
        "            'CreditLinesUtilization', 'HighRiskPastDue', 'RealEstateRatio', 'IncomePerDependent']\n",
        "print(f\"\\nCreated {len(new_vars)} additional variables:\")\n",
        "for var in new_vars:\n",
        "    print(f\"  - {var}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"NEW VARIABLES DESCRIPTIVE STATISTICS\")\n",
        "print(\"=\" * 80)\n",
        "print(df_train_features[new_vars].describe())\n",
        "\n",
        "# Check correlation of new variables with target\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"CORRELATION OF NEW VARIABLES WITH TARGET\")\n",
        "print(\"=\" * 80)\n",
        "corr_new = df_train_features[new_vars + ['SeriousDlqin2yrs']].corr()['SeriousDlqin2yrs'].sort_values(ascending=False)\n",
        "print(corr_new)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLgsHhPCYJLK"
      },
      "source": [
        "# 3. Model building - (Task weight: 50%)\n",
        "A logistic regression must be built on the WoE variables.\n",
        "\n",
        "- If any other model is built, the score is 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLbc0tssuWM-"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and validation sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Prepare features and target from feature-engineered data (cell 10)\n",
        "feature_cols = [\n",
        "    'RevolvingUtilizationOfUnsecuredLines', 'age',\n",
        "    'NumberOfTime30-59DaysPastDueNotWorse', 'DebtRatio',\n",
        "    'MonthlyIncome_filled', 'NumberOfOpenCreditLinesAndLoans',\n",
        "    'NumberOfTimes90DaysLate', 'NumberRealEstateLoansOrLines',\n",
        "    'NumberOfTime60-89DaysPastDueNotWorse', 'NumberOfDependents_filled',\n",
        "    'TotalPastDue', 'UtilizationToIncomeRatio', 'DebtServiceRatio',\n",
        "    'CreditLinesUtilization', 'HighRiskPastDue', 'RealEstateRatio', 'IncomePerDependent'\n",
        "]\n",
        "\n",
        "X = df_train_features[feature_cols].copy()\n",
        "y = df_train_features['SeriousDlqin2yrs'].copy()\n",
        "\n",
        "# Split into train and validation sets (80/20 split with stratification)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"DATA SPLIT SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Training set: {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"Validation set: {X_val.shape[0]:,} samples ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"\\nTraining target distribution:\")\n",
        "print(y_train.value_counts())\n",
        "print(f\"\\nValidation target distribution:\")\n",
        "print(y_val.value_counts())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moF8yQ5Yrge5"
      },
      "source": [
        "A WoE transformation must be calculated - maximum 3 points.\n",
        "\n",
        "The WoE calculation must be done in two steps:\n",
        "\n",
        "Step 1. Fine Classification (1 point). Splitting into a large number of bins (intervals).\n",
        "- Typically, interval variables are divided into 20, 30, and so on intervals.\n",
        "- For categorical variables, one category per group.\n",
        "- Next, calculate the WoE for each group.\n",
        "\n",
        "Step 2. Coarse Classification (2 points). Consolidating the intervals obtained in Step 1. The result should be no more than 5-10 intervals.\n",
        "- Typically, groups with similar WoE values ​​are combined.\n",
        "- The WoE must be monotonic, meaning that after your combination, the result must be interpretable (it is necessary to graphically demonstrate that the WoE is monotonic).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-15T12:35:40.235256Z",
          "start_time": "2020-10-15T12:35:40.226211Z"
        },
        "id": "UMmiEzvVYJLK",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [],
      "source": [
        "# WoE (Weight of Evidence) Transformation\n",
        "# Step 1: Fine Classification - Split into many bins (20 bins)\n",
        "# Step 2: Coarse Classification - Combine bins to ensure monotonicity (max 5 bins)\n",
        "\n",
        "def calculate_woe_iv(X_data, y_data, feature, n_bins=20):\n",
        "    \"\"\"\n",
        "    Calculate WoE and IV for a feature\n",
        "    Returns: grouped DataFrame with bin statistics, WoE dictionary, and IV score\n",
        "    \"\"\"\n",
        "    # Create a copy and handle missing values\n",
        "    data = pd.DataFrame({feature: X_data[feature].fillna(X_data[feature].median()), \n",
        "                         'target': y_data})\n",
        "    \n",
        "    # Create bins using quantiles (fine classification)\n",
        "    try:\n",
        "        data['bin'] = pd.qcut(data[feature], q=n_bins, duplicates='drop')\n",
        "    except:\n",
        "        # If quantile cut fails, use equal-width bins\n",
        "        data['bin'] = pd.cut(data[feature], bins=n_bins, duplicates='drop')\n",
        "    \n",
        "    # Calculate statistics for each bin\n",
        "    grouped = data.groupby('bin', observed=True).agg({\n",
        "        'target': ['count', 'sum', 'mean']\n",
        "    })\n",
        "    grouped.columns = ['Total', 'Bad', 'Bad_Rate']\n",
        "    grouped['Good'] = grouped['Total'] - grouped['Bad']\n",
        "    \n",
        "    # Calculate percentages\n",
        "    total_good = grouped['Good'].sum()\n",
        "    total_bad = grouped['Bad'].sum()\n",
        "    \n",
        "    if total_good == 0 or total_bad == 0:\n",
        "        return None, None, None\n",
        "    \n",
        "    grouped['Good_Pct'] = grouped['Good'] / total_good\n",
        "    grouped['Bad_Pct'] = grouped['Bad'] / total_bad\n",
        "    \n",
        "    # Calculate WoE: log(Good_Pct / Bad_Pct)\n",
        "    grouped['WoE'] = np.log((grouped['Good_Pct'] + 1e-6) / (grouped['Bad_Pct'] + 1e-6))\n",
        "    \n",
        "    # Calculate IV (Information Value)\n",
        "    grouped['IV'] = (grouped['Good_Pct'] - grouped['Bad_Pct']) * grouped['WoE']\n",
        "    iv_total = grouped['IV'].sum()\n",
        "    \n",
        "    # Extract bin boundaries\n",
        "    grouped['Bin_Min'] = grouped.index.map(lambda x: x.left if hasattr(x, 'left') else float('-inf'))\n",
        "    grouped['Bin_Max'] = grouped.index.map(lambda x: x.right if hasattr(x, 'right') else float('inf'))\n",
        "    \n",
        "    return grouped, grouped['WoE'].to_dict(), iv_total\n",
        "\n",
        "def coarse_classification(grouped, max_bins=5):\n",
        "    \"\"\"\n",
        "    Combine bins to ensure monotonic WoE (coarse classification)\n",
        "    Groups adjacent bins with similar WoE values\n",
        "    \"\"\"\n",
        "    grouped_sorted = grouped.sort_values('Bin_Min').copy()\n",
        "    woe_values = grouped_sorted['WoE'].values\n",
        "    \n",
        "    # Group bins with similar WoE values\n",
        "    bins_list = []\n",
        "    current_group = [0]\n",
        "    \n",
        "    for i in range(1, len(woe_values)):\n",
        "        # Combine if WoE difference is small (< 0.1)\n",
        "        if abs(woe_values[i] - woe_values[current_group[-1]]) < 0.1:\n",
        "            current_group.append(i)\n",
        "        else:\n",
        "            bins_list.append(current_group)\n",
        "            current_group = [i]\n",
        "    bins_list.append(current_group)\n",
        "    \n",
        "    # Reduce to max_bins by merging smallest groups\n",
        "    while len(bins_list) > max_bins:\n",
        "        sizes = [len(b) for b in bins_list]\n",
        "        min_idx = sizes.index(min(sizes))\n",
        "        \n",
        "        # Merge with neighbor\n",
        "        if min_idx == 0:\n",
        "            bins_list[0] = bins_list[0] + bins_list[1]\n",
        "            bins_list.pop(1)\n",
        "        elif min_idx == len(bins_list) - 1:\n",
        "            bins_list[-2] = bins_list[-2] + bins_list[-1]\n",
        "            bins_list.pop(-1)\n",
        "        else:\n",
        "            # Merge with closer neighbor (by WoE similarity)\n",
        "            left_woe = grouped_sorted.iloc[bins_list[min_idx-1][-1]]['WoE']\n",
        "            right_woe = grouped_sorted.iloc[bins_list[min_idx+1][0]]['WoE']\n",
        "            current_woe = grouped_sorted.iloc[bins_list[min_idx][0]]['WoE']\n",
        "            \n",
        "            if abs(current_woe - left_woe) < abs(current_woe - right_woe):\n",
        "                bins_list[min_idx-1] = bins_list[min_idx-1] + bins_list[min_idx]\n",
        "            else:\n",
        "                bins_list[min_idx] = bins_list[min_idx] + bins_list[min_idx+1]\n",
        "                bins_list.pop(min_idx+1)\n",
        "            bins_list.pop(min_idx)\n",
        "    \n",
        "    # Create new grouped dataframe\n",
        "    coarse_groups = []\n",
        "    for bin_group in bins_list:\n",
        "        group_data = grouped_sorted.iloc[bin_group]\n",
        "        coarse_groups.append({\n",
        "            'Total': group_data['Total'].sum(),\n",
        "            'Bad': group_data['Bad'].sum(),\n",
        "            'Good': group_data['Good'].sum(),\n",
        "            'Bad_Rate': group_data['Bad'].sum() / group_data['Total'].sum() if group_data['Total'].sum() > 0 else 0,\n",
        "            'Bin_Min': group_data['Bin_Min'].min(),\n",
        "            'Bin_Max': group_data['Bin_Max'].max()\n",
        "        })\n",
        "    \n",
        "    coarse_df = pd.DataFrame(coarse_groups)\n",
        "    total_good = coarse_df['Good'].sum()\n",
        "    total_bad = coarse_df['Bad'].sum()\n",
        "    \n",
        "    # Recalculate WoE for combined bins\n",
        "    coarse_df['Good_Pct'] = coarse_df['Good'] / total_good\n",
        "    coarse_df['Bad_Pct'] = coarse_df['Bad'] / total_bad\n",
        "    coarse_df['WoE'] = np.log((coarse_df['Good_Pct'] + 1e-6) / (coarse_df['Bad_Pct'] + 1e-6))\n",
        "    coarse_df['IV'] = (coarse_df['Good_Pct'] - coarse_df['Bad_Pct']) * coarse_df['WoE']\n",
        "    \n",
        "    return coarse_df\n",
        "\n",
        "# Step 1: Fine Classification\n",
        "print(\"=\" * 80)\n",
        "print(\"STEP 1: FINE CLASSIFICATION (20 bins per feature)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Select numerical features for WoE transformation\n",
        "woe_features = [\n",
        "    'RevolvingUtilizationOfUnsecuredLines', 'age', 'DebtRatio',\n",
        "    'MonthlyIncome_filled', 'NumberOfOpenCreditLinesAndLoans',\n",
        "    'NumberRealEstateLoansOrLines', 'NumberOfDependents_filled',\n",
        "    'TotalPastDue', 'UtilizationToIncomeRatio', 'DebtServiceRatio',\n",
        "    'CreditLinesUtilization', 'RealEstateRatio', 'IncomePerDependent'\n",
        "]\n",
        "\n",
        "fine_classification_results = {}\n",
        "iv_scores = {}\n",
        "\n",
        "for feature in woe_features:\n",
        "    if feature in X_train.columns:\n",
        "        grouped, woe_map, iv = calculate_woe_iv(X_train, y_train, feature, n_bins=20)\n",
        "        if grouped is not None:\n",
        "            fine_classification_results[feature] = grouped\n",
        "            iv_scores[feature] = iv\n",
        "            print(f\"{feature:35s} - IV: {iv:.4f}, Bins: {len(grouped)}\")\n",
        "\n",
        "# Display IV scores\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"INFORMATION VALUE (IV) SCORES\")\n",
        "print(\"=\" * 80)\n",
        "iv_df = pd.DataFrame(list(iv_scores.items()), columns=['Feature', 'IV'])\n",
        "iv_df = iv_df.sort_values('IV', ascending=False)\n",
        "print(iv_df.to_string(index=False))\n",
        "\n",
        "# Step 2: Coarse Classification\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 2: COARSE CLASSIFICATION (max 5 bins per feature)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "coarse_classification_results = {}\n",
        "\n",
        "for feature in woe_features:\n",
        "    if feature in fine_classification_results:\n",
        "        coarse_df = coarse_classification(fine_classification_results[feature], max_bins=5)\n",
        "        coarse_classification_results[feature] = coarse_df\n",
        "        print(f\"\\n{feature}: {len(coarse_df)} bins\")\n",
        "        print(coarse_df[['Bin_Min', 'Bin_Max', 'Total', 'Bad', 'Bad_Rate', 'WoE']].round(4))\n",
        "\n",
        "# Visualize WoE monotonicity\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"VISUALIZING WoE MONOTONICITY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "top_features = iv_df.head(6)['Feature'].tolist()\n",
        "n_plots = min(len(top_features), 6)\n",
        "n_cols = 3\n",
        "n_rows = (n_plots + n_cols - 1) // n_cols\n",
        "\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 5 * n_rows))\n",
        "if n_plots == 1:\n",
        "    axes = [axes]\n",
        "else:\n",
        "    axes = axes.flatten()\n",
        "\n",
        "for idx, feature in enumerate(top_features[:n_plots]):\n",
        "    if feature in coarse_classification_results:\n",
        "        coarse_df = coarse_classification_results[feature]\n",
        "        bin_centers = (coarse_df['Bin_Min'] + coarse_df['Bin_Max']) / 2\n",
        "        \n",
        "        axes[idx].plot(bin_centers, coarse_df['WoE'], marker='o', linewidth=2, markersize=8)\n",
        "        axes[idx].set_title(f'{feature}\\nWoE Monotonicity', fontsize=11, fontweight='bold')\n",
        "        axes[idx].set_xlabel('Feature Value (Bin Center)', fontsize=10)\n",
        "        axes[idx].set_ylabel('WoE', fontsize=10)\n",
        "        axes[idx].grid(True, alpha=0.3)\n",
        "        axes[idx].axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
        "\n",
        "# Hide unused subplots\n",
        "for idx in range(n_plots, len(axes)):\n",
        "    axes[idx].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Apply WoE transformation\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"APPLYING WoE TRANSFORMATION TO DATA\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "def apply_woe_transformation(X_data, feature, coarse_df):\n",
        "    \"\"\"Apply WoE transformation to a feature using coarse classification bins\"\"\"\n",
        "    feature_values = X_data[feature].fillna(X_data[feature].median())\n",
        "    \n",
        "    # Create bins based on coarse classification boundaries\n",
        "    bins = [-np.inf] + coarse_df['Bin_Max'].tolist()[:-1] + [np.inf]\n",
        "    labels = range(len(coarse_df))\n",
        "    \n",
        "    binned = pd.cut(feature_values, bins=bins, labels=labels, include_lowest=True)\n",
        "    \n",
        "    # Map to WoE values\n",
        "    woe_mapping = dict(zip(labels, coarse_df['WoE'].values))\n",
        "    woe_values = binned.map(woe_mapping)\n",
        "    \n",
        "    return woe_values\n",
        "\n",
        "# Apply WoE to training and validation sets\n",
        "X_train_woe = X_train.copy()\n",
        "X_val_woe = X_val.copy()\n",
        "\n",
        "for feature in woe_features:\n",
        "    if feature in coarse_classification_results:\n",
        "        X_train_woe[f'{feature}_WoE'] = apply_woe_transformation(\n",
        "            X_train, feature, coarse_classification_results[feature]\n",
        "        )\n",
        "        X_val_woe[f'{feature}_WoE'] = apply_woe_transformation(\n",
        "            X_val, feature, coarse_classification_results[feature]\n",
        "        )\n",
        "        print(f\"✓ {feature}\")\n",
        "\n",
        "# Prepare final feature sets for modeling\n",
        "woe_feature_cols = [f'{f}_WoE' for f in woe_features if f in coarse_classification_results]\n",
        "binary_features = ['HighRiskPastDue', 'NumberOfTime30-59DaysPastDueNotWorse', \n",
        "                   'NumberOfTime60-89DaysPastDueNotWorse', 'NumberOfTimes90DaysLate']\n",
        "\n",
        "# Select only features that exist in the data\n",
        "available_binary = [f for f in binary_features if f in X_train_woe.columns]\n",
        "\n",
        "X_train_final = X_train_woe[woe_feature_cols + available_binary].copy()\n",
        "X_val_final = X_val_woe[woe_feature_cols + available_binary].copy()\n",
        "\n",
        "print(f\"\\nFinal feature set: {len(X_train_final.columns)} features\")\n",
        "print(f\"Features: {X_train_final.columns.tolist()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_gU5MsDtlfU"
      },
      "source": [
        "Constructing and evaluating logistic regression - maximum 1 point\n",
        "- Constructing only logistic regression - 0.3 points\n",
        "- Evaluating the model (roc auc, f1, etc.) - 0.3 points\n",
        "- Constructing a scorecard - 0.4 points\n",
        "\n",
        "As a reminder, the following formulas are required for the scorecard (details in the lecture and seminar):\n",
        "Score_i =  (βi × WoE_i + α/n) × Factor + Offset/n, где\n",
        "\n",
        "- Factor = pdo/ln(2)\n",
        "\n",
        "- Offset = Target Score — (Factor × ln(Target Odds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KO1Tkg7k1Kq_"
      },
      "outputs": [],
      "source": [
        "# Building Logistic Regression Model on WoE Variables\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, f1_score, precision_score, recall_score, confusion_matrix, classification_report\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"LOGISTIC REGRESSION MODEL\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Handle any remaining missing values\n",
        "X_train_final = X_train_final.fillna(0)\n",
        "X_val_final = X_val_final.fillna(0)\n",
        "\n",
        "# Build logistic regression model\n",
        "lr_model = LogisticRegression(random_state=42, max_iter=1000, solver='lbfgs')\n",
        "lr_model.fit(X_train_final, y_train)\n",
        "\n",
        "print(f\"✓ Model trained with {len(X_train_final.columns)} features\")\n",
        "\n",
        "# Make predictions\n",
        "y_train_pred_proba = lr_model.predict_proba(X_train_final)[:, 1]\n",
        "y_val_pred_proba = lr_model.predict_proba(X_val_final)[:, 1]\n",
        "y_train_pred = lr_model.predict(X_train_final)\n",
        "y_val_pred = lr_model.predict(X_val_final)\n",
        "\n",
        "# Model Evaluation\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"MODEL EVALUATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ROC AUC Score\n",
        "train_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
        "val_auc = roc_auc_score(y_val, y_val_pred_proba)\n",
        "\n",
        "print(f\"\\nROC AUC Score:\")\n",
        "print(f\"  Training: {train_auc:.4f}\")\n",
        "print(f\"  Validation: {val_auc:.4f}\")\n",
        "\n",
        "# F1 Score\n",
        "train_f1 = f1_score(y_train, y_train_pred)\n",
        "val_f1 = f1_score(y_val, y_val_pred)\n",
        "\n",
        "print(f\"\\nF1 Score:\")\n",
        "print(f\"  Training: {train_f1:.4f}\")\n",
        "print(f\"  Validation: {val_f1:.4f}\")\n",
        "\n",
        "# Precision and Recall\n",
        "train_precision = precision_score(y_train, y_train_pred)\n",
        "train_recall = recall_score(y_train, y_train_pred)\n",
        "val_precision = precision_score(y_val, y_val_pred)\n",
        "val_recall = recall_score(y_val, y_val_pred)\n",
        "\n",
        "print(f\"\\nPrecision:\")\n",
        "print(f\"  Training: {train_precision:.4f}\")\n",
        "print(f\"  Validation: {val_precision:.4f}\")\n",
        "\n",
        "print(f\"\\nRecall:\")\n",
        "print(f\"  Training: {train_recall:.4f}\")\n",
        "print(f\"  Validation: {val_recall:.4f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "print(f\"\\nConfusion Matrix (Validation):\")\n",
        "cm_val = confusion_matrix(y_val, y_val_pred)\n",
        "print(cm_val)\n",
        "print(f\"\\nTrue Negatives: {cm_val[0,0]}, False Positives: {cm_val[0,1]}\")\n",
        "print(f\"False Negatives: {cm_val[1,0]}, True Positives: {cm_val[1,1]}\")\n",
        "\n",
        "# Classification Report\n",
        "print(f\"\\nClassification Report (Validation):\")\n",
        "print(classification_report(y_val, y_val_pred, target_names=['No Default', 'Default']))\n",
        "\n",
        "# ROC Curve\n",
        "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
        "fpr_val, tpr_val, _ = roc_curve(y_val, y_val_pred_proba)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot(fpr_train, tpr_train, label=f'Training ROC (AUC = {train_auc:.4f})', linewidth=2)\n",
        "plt.plot(fpr_val, tpr_val, label=f'Validation ROC (AUC = {val_auc:.4f})', linewidth=2)\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
        "plt.xlabel('False Positive Rate', fontsize=12)\n",
        "plt.ylabel('True Positive Rate', fontsize=12)\n",
        "plt.title('ROC Curve - Logistic Regression', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc='lower right', fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Scorecard Construction\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"SCORECARD CONSTRUCTION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Scorecard parameters\n",
        "pdo = 20  # Points to Double Odds (standard: 20 points doubles the odds)\n",
        "target_score = 600  # Target score at target odds\n",
        "target_odds = 50  # Target odds (e.g., 50:1 means 1 default per 50 good)\n",
        "\n",
        "# Calculate Factor and Offset\n",
        "factor = pdo / np.log(2)\n",
        "offset = target_score - (factor * np.log(target_odds))\n",
        "\n",
        "print(f\"Scorecard Parameters:\")\n",
        "print(f\"  PDO (Points to Double Odds): {pdo}\")\n",
        "print(f\"  Target Score: {target_score}\")\n",
        "print(f\"  Target Odds: {target_odds}:1\")\n",
        "print(f\"  Factor: {factor:.4f}\")\n",
        "print(f\"  Offset: {offset:.4f}\")\n",
        "\n",
        "# Get model coefficients\n",
        "coefficients = lr_model.coef_[0]\n",
        "intercept = lr_model.intercept_[0]\n",
        "n_features = len(coefficients)\n",
        "\n",
        "print(f\"\\nModel Intercept (α): {intercept:.4f}\")\n",
        "print(f\"Number of features (n): {n_features}\")\n",
        "\n",
        "# Calculate score for each feature\n",
        "scorecard = pd.DataFrame({\n",
        "    'Feature': X_train_final.columns,\n",
        "    'Coefficient': coefficients,\n",
        "    'Factor_Component': coefficients * factor,\n",
        "    'Offset_Component': offset / n_features\n",
        "})\n",
        "\n",
        "scorecard['Points'] = scorecard['Factor_Component'] + scorecard['Offset_Component']\n",
        "scorecard = scorecard.sort_values('Points', ascending=False)\n",
        "\n",
        "print(f\"\\nScorecard (Points per feature):\")\n",
        "print(scorecard[['Feature', 'Coefficient', 'Points']].to_string(index=False))\n",
        "\n",
        "# Calculate base score (from intercept)\n",
        "base_score = (intercept / n_features) * factor + (offset / n_features)\n",
        "print(f\"\\nBase Score (from intercept): {base_score:.2f}\")\n",
        "\n",
        "# Example: Calculate score for a sample observation\n",
        "sample_idx = 0\n",
        "sample_woe = X_train_final.iloc[sample_idx]\n",
        "sample_score = base_score + sample_woe.dot(scorecard.set_index('Feature')['Points'])\n",
        "sample_prob = lr_model.predict_proba(X_train_final.iloc[sample_idx:sample_idx+1])[0, 1]\n",
        "\n",
        "print(f\"\\nExample Score Calculation:\")\n",
        "print(f\"  Sample observation index: {sample_idx}\")\n",
        "print(f\"  Predicted probability: {sample_prob:.4f}\")\n",
        "print(f\"  Calculated score: {sample_score:.2f}\")\n",
        "\n",
        "# Score distribution\n",
        "train_scores = base_score + X_train_final.dot(scorecard.set_index('Feature')['Points'])\n",
        "val_scores = base_score + X_val_final.dot(scorecard.set_index('Feature')['Points'])\n",
        "\n",
        "plt.figure(figsize=(14, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(train_scores[y_train == 0], bins=50, alpha=0.7, label='No Default', color='skyblue', edgecolor='black')\n",
        "plt.hist(train_scores[y_train == 1], bins=50, alpha=0.7, label='Default', color='salmon', edgecolor='black')\n",
        "plt.xlabel('Score', fontsize=12)\n",
        "plt.ylabel('Frequency', fontsize=12)\n",
        "plt.title('Score Distribution - Training Set', fontsize=14, fontweight='bold')\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(val_scores[y_val == 0], bins=50, alpha=0.7, label='No Default', color='skyblue', edgecolor='black')\n",
        "plt.hist(val_scores[y_val == 1], bins=50, alpha=0.7, label='Default', color='salmon', edgecolor='black')\n",
        "plt.xlabel('Score', fontsize=12)\n",
        "plt.ylabel('Frequency', fontsize=12)\n",
        "plt.title('Score Distribution - Validation Set', fontsize=14, fontweight='bold')\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nScore Statistics:\")\n",
        "print(f\"  Training - Mean: {train_scores.mean():.2f}, Std: {train_scores.std():.2f}\")\n",
        "print(f\"  Training - Min: {train_scores.min():.2f}, Max: {train_scores.max():.2f}\")\n",
        "print(f\"  Validation - Mean: {val_scores.mean():.2f}, Std: {val_scores.std():.2f}\")\n",
        "print(f\"  Validation - Min: {val_scores.min():.2f}, Max: {val_scores.max():.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Population Stability Index (PSI) Analysis\n",
        "# PSI measures the stability of variable distributions between train and test sets\n",
        "\n",
        "def calculate_psi(expected, actual, bins=10):\n",
        "    \"\"\"\n",
        "    Calculate Population Stability Index (PSI)\n",
        "    PSI < 0.1: No significant change\n",
        "    0.1 <= PSI < 0.25: Moderate change\n",
        "    PSI >= 0.25: Significant change\n",
        "    \"\"\"\n",
        "    # Create bins based on expected distribution\n",
        "    breakpoints = np.linspace(expected.min(), expected.max(), bins + 1)\n",
        "    breakpoints[0] = -np.inf\n",
        "    breakpoints[-1] = np.inf\n",
        "    \n",
        "    # Calculate distributions\n",
        "    expected_pct = pd.cut(expected, breakpoints).value_counts(normalize=True, sort=False)\n",
        "    actual_pct = pd.cut(actual, breakpoints).value_counts(normalize=True, sort=False)\n",
        "    \n",
        "    # Handle zero values\n",
        "    expected_pct = expected_pct + 1e-6\n",
        "    actual_pct = actual_pct + 1e-6\n",
        "    \n",
        "    # Calculate PSI\n",
        "    psi = np.sum((actual_pct - expected_pct) * np.log(actual_pct / expected_pct))\n",
        "    \n",
        "    return psi\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"POPULATION STABILITY INDEX (PSI) ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "print(\"Comparing df_train_features (expected) vs df_test_features (actual)\")\n",
        "\n",
        "# Use features from feature engineering (cell 10)\n",
        "# Select numerical features for PSI calculation\n",
        "psi_features = [\n",
        "    'RevolvingUtilizationOfUnsecuredLines', 'age', 'DebtRatio',\n",
        "    'MonthlyIncome_filled', 'NumberOfOpenCreditLinesAndLoans',\n",
        "    'NumberRealEstateLoansOrLines', 'NumberOfDependents_filled',\n",
        "    'TotalPastDue', 'UtilizationToIncomeRatio', 'DebtServiceRatio',\n",
        "    'CreditLinesUtilization', 'RealEstateRatio', 'IncomePerDependent'\n",
        "]\n",
        "\n",
        "psi_results = {}\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"PSI RESULTS BY FEATURE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for feature in psi_features:\n",
        "    if feature in df_train_features.columns and feature in df_test_features.columns:\n",
        "        # Handle missing values\n",
        "        train_vals = df_train_features[feature].fillna(df_train_features[feature].median())\n",
        "        test_vals = df_test_features[feature].fillna(df_test_features[feature].median())\n",
        "        \n",
        "        # Calculate PSI\n",
        "        psi = calculate_psi(train_vals, test_vals, bins=10)\n",
        "        psi_results[feature] = psi\n",
        "        \n",
        "        # Interpret PSI\n",
        "        if psi < 0.1:\n",
        "            interpretation = \"No significant change\"\n",
        "        elif psi < 0.25:\n",
        "            interpretation = \"Moderate change\"\n",
        "        else:\n",
        "            interpretation = \"Significant change\"\n",
        "        \n",
        "        print(f\"{feature:35s} PSI: {psi:7.4f} - {interpretation}\")\n",
        "\n",
        "# Create summary DataFrame\n",
        "psi_df = pd.DataFrame(list(psi_results.items()), columns=['Feature', 'PSI'])\n",
        "psi_df = psi_df.sort_values('PSI', ascending=False)\n",
        "\n",
        "# Categorize PSI values\n",
        "psi_df['Stability'] = psi_df['PSI'].apply(lambda x: \n",
        "    'No change' if x < 0.1 else ('Moderate change' if x < 0.25 else 'Significant change'))\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"PSI SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "print(psi_df.to_string(index=False))\n",
        "\n",
        "# Visualize PSI results\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# PSI values bar chart\n",
        "axes[0].barh(range(len(psi_df)), psi_df['PSI'], color=['green' if x < 0.1 else ('orange' if x < 0.25 else 'red') \n",
        "                                                         for x in psi_df['PSI']])\n",
        "axes[0].set_yticks(range(len(psi_df)))\n",
        "axes[0].set_yticklabels(psi_df['Feature'], fontsize=9)\n",
        "axes[0].set_xlabel('PSI Value', fontsize=12)\n",
        "axes[0].set_title('PSI Values by Feature', fontsize=14, fontweight='bold')\n",
        "axes[0].axvline(x=0.1, color='orange', linestyle='--', label='Moderate threshold')\n",
        "axes[0].axvline(x=0.25, color='red', linestyle='--', label='Significant threshold')\n",
        "axes[0].legend(fontsize=10)\n",
        "axes[0].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# Stability categories\n",
        "stability_counts = psi_df['Stability'].value_counts()\n",
        "axes[1].bar(stability_counts.index, stability_counts.values, \n",
        "            color=['green', 'orange', 'red'][:len(stability_counts)])\n",
        "axes[1].set_ylabel('Number of Features', fontsize=12)\n",
        "axes[1].set_title('Stability Categories', fontsize=14, fontweight='bold')\n",
        "axes[1].grid(True, alpha=0.3, axis='y')\n",
        "for i, v in enumerate(stability_counts.values):\n",
        "    axes[1].text(i, v + 0.5, str(v), ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Overall assessment\n",
        "high_psi_features = psi_df[psi_df['PSI'] >= 0.25]\n",
        "moderate_psi_features = psi_df[(psi_df['PSI'] >= 0.1) & (psi_df['PSI'] < 0.25)]\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"OVERALL ASSESSMENT\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Features with significant change (PSI >= 0.25): {len(high_psi_features)}\")\n",
        "if len(high_psi_features) > 0:\n",
        "    print(\"  \" + \", \".join(high_psi_features['Feature'].tolist()))\n",
        "\n",
        "print(f\"\\nFeatures with moderate change (0.1 <= PSI < 0.25): {len(moderate_psi_features)}\")\n",
        "if len(moderate_psi_features) > 0:\n",
        "    print(\"  \" + \", \".join(moderate_psi_features['Feature'].tolist()))\n",
        "\n",
        "print(f\"\\nFeatures with no significant change (PSI < 0.1): {len(psi_df[psi_df['PSI'] < 0.1])}\")\n",
        "\n",
        "if len(high_psi_features) > 0:\n",
        "    print(\"\\n⚠️  WARNING: Some features show significant distribution changes between train and test sets.\")\n",
        "    print(\"   This may indicate data drift and could affect model performance.\")\n",
        "else:\n",
        "    print(\"\\n✓ Overall, the distributions are relatively stable between train and test sets.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avUP2rHjMdNJ"
      },
      "source": [
        "Conduct a sample stability analysis using PSI\n",
        "- Compare the test and training samples you downloaded from Kaggle (df_train, df_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qx4DuBocvS9T"
      },
      "source": [
        "# 4. Using methods to reduce class imbalance - (Task weight: 20%)\n",
        "- Try several methods to reduce class imbalance\n",
        "- Choose the one that brings the greatest improvement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmbXZxHMysqF"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'imblearn'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Methods to Reduce Class Imbalance\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# We'll try several methods and compare their performance\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimblearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mover_sampling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SMOTE, RandomOverSampler, ADASYN\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimblearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01munder_sampling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RandomUnderSampler\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimblearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcombine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SMOTETomek, SMOTEENN\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'imblearn'"
          ]
        }
      ],
      "source": [
        "# Methods to Reduce Class Imbalance\n",
        "# Try several methods and compare their performance\n",
        "\n",
        "from imblearn.over_sampling import SMOTE, RandomOverSampler, ADASYN\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.combine import SMOTETomek\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from collections import Counter\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"CLASS IMBALANCE REDUCTION METHODS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Original class distribution\n",
        "print(\"\\nOriginal class distribution:\")\n",
        "print(f\"  Class 0 (No Default): {Counter(y_train)[0]}\")\n",
        "print(f\"  Class 1 (Default): {Counter(y_train)[1]}\")\n",
        "print(f\"  Imbalance Ratio: {Counter(y_train)[0] / Counter(y_train)[1]:.2f}:1\")\n",
        "\n",
        "# Prepare data for resampling\n",
        "X_train_resample = X_train_final.fillna(0).copy()\n",
        "y_train_resample = y_train.copy()\n",
        "X_val_resample = X_val_final.fillna(0).copy()\n",
        "\n",
        "# Method 1: Random Over-Sampling\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"METHOD 1: Random Over-Sampling\")\n",
        "print(\"=\" * 80)\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_ros, y_ros = ros.fit_resample(X_train_resample, y_train_resample)\n",
        "print(f\"After resampling - Class 0: {Counter(y_ros)[0]}, Class 1: {Counter(y_ros)[1]}\")\n",
        "\n",
        "# Method 2: SMOTE (Synthetic Minority Over-sampling Technique)\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"METHOD 2: SMOTE\")\n",
        "print(\"=\" * 80)\n",
        "try:\n",
        "    smote = SMOTE(random_state=42, k_neighbors=5)\n",
        "    X_smote, y_smote = smote.fit_resample(X_train_resample, y_train_resample)\n",
        "    print(f\"After resampling - Class 0: {Counter(y_smote)[0]}, Class 1: {Counter(y_smote)[1]}\")\n",
        "except Exception as e:\n",
        "    print(f\"SMOTE failed: {e}\")\n",
        "    X_smote, y_smote = X_train_resample, y_train_resample\n",
        "\n",
        "# Method 3: ADASYN (Adaptive Synthetic Sampling)\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"METHOD 3: ADASYN\")\n",
        "print(\"=\" * 80)\n",
        "try:\n",
        "    adasyn = ADASYN(random_state=42, n_neighbors=5)\n",
        "    X_adasyn, y_adasyn = adasyn.fit_resample(X_train_resample, y_train_resample)\n",
        "    print(f\"After resampling - Class 0: {Counter(y_adasyn)[0]}, Class 1: {Counter(y_adasyn)[1]}\")\n",
        "except Exception as e:\n",
        "    print(f\"ADASYN failed: {e}\")\n",
        "    X_adasyn, y_adasyn = X_train_resample, y_train_resample\n",
        "\n",
        "# Method 4: Random Under-Sampling\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"METHOD 4: Random Under-Sampling\")\n",
        "print(\"=\" * 80)\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "X_rus, y_rus = rus.fit_resample(X_train_resample, y_train_resample)\n",
        "print(f\"After resampling - Class 0: {Counter(y_rus)[0]}, Class 1: {Counter(y_rus)[1]}\")\n",
        "\n",
        "# Method 5: SMOTE + Tomek Links (Combination)\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"METHOD 5: SMOTE + Tomek Links\")\n",
        "print(\"=\" * 80)\n",
        "try:\n",
        "    smote_tomek = SMOTETomek(random_state=42)\n",
        "    X_smote_tomek, y_smote_tomek = smote_tomek.fit_resample(X_train_resample, y_train_resample)\n",
        "    print(f\"After resampling - Class 0: {Counter(y_smote_tomek)[0]}, Class 1: {Counter(y_smote_tomek)[1]}\")\n",
        "except Exception as e:\n",
        "    print(f\"SMOTE+Tomek failed: {e}\")\n",
        "    X_smote_tomek, y_smote_tomek = X_train_resample, y_train_resample\n",
        "\n",
        "# Method 6: Class Weight Balancing (in model)\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"METHOD 6: Class Weight Balancing (in Logistic Regression)\")\n",
        "print(\"=\" * 80)\n",
        "# Calculate class weights\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
        "print(f\"Class weights: {class_weight_dict}\")\n",
        "\n",
        "# Train and evaluate models with different resampling methods\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"TRAINING AND EVALUATING MODELS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "methods = {\n",
        "    'Original (No Resampling)': (X_train_resample, y_train_resample, False),\n",
        "    'Random Over-Sampling': (X_ros, y_ros, False),\n",
        "    'SMOTE': (X_smote, y_smote, False),\n",
        "    'ADASYN': (X_adasyn, y_adasyn, False),\n",
        "    'Random Under-Sampling': (X_rus, y_rus, False),\n",
        "    'SMOTE + Tomek': (X_smote_tomek, y_smote_tomek, False),\n",
        "    'Class Weight Balancing': (X_train_resample, y_train_resample, True)\n",
        "}\n",
        "\n",
        "results = []\n",
        "\n",
        "for method_name, (X_method, y_method, use_class_weight) in methods.items():\n",
        "    print(f\"\\n{method_name}:\")\n",
        "    \n",
        "    # Train model\n",
        "    if use_class_weight:\n",
        "        lr_method = LogisticRegression(random_state=42, max_iter=1000, solver='lbfgs', \n",
        "                                       class_weight='balanced')\n",
        "    else:\n",
        "        lr_method = LogisticRegression(random_state=42, max_iter=1000, solver='lbfgs')\n",
        "    \n",
        "    lr_method.fit(X_method, y_method)\n",
        "    \n",
        "    # Predictions\n",
        "    y_val_pred_proba = lr_method.predict_proba(X_val_resample)[:, 1]\n",
        "    y_val_pred = lr_method.predict(X_val_resample)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    metrics = {\n",
        "        'Method': method_name,\n",
        "        'AUC': roc_auc_score(y_val, y_val_pred_proba),\n",
        "        'F1': f1_score(y_val, y_val_pred),\n",
        "        'Precision': precision_score(y_val, y_val_pred),\n",
        "        'Recall': recall_score(y_val, y_val_pred)\n",
        "    }\n",
        "    results.append(metrics)\n",
        "    \n",
        "    print(f\"  AUC: {metrics['AUC']:.4f}, F1: {metrics['F1']:.4f}, \"\n",
        "          f\"Precision: {metrics['Precision']:.4f}, Recall: {metrics['Recall']:.4f}\")\n",
        "\n",
        "# Compare results\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df = results_df.sort_values('AUC', ascending=False)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"COMPARISON OF METHODS\")\n",
        "print(\"=\" * 80)\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "# Visualize comparison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# AUC Comparison\n",
        "axes[0, 0].barh(results_df['Method'], results_df['AUC'], color='skyblue', edgecolor='black')\n",
        "axes[0, 0].set_xlabel('ROC AUC Score', fontsize=12)\n",
        "axes[0, 0].set_title('ROC AUC Comparison', fontsize=14, fontweight='bold')\n",
        "axes[0, 0].grid(True, alpha=0.3, axis='x')\n",
        "axes[0, 0].axvline(x=results_df['AUC'].max(), color='red', linestyle='--', alpha=0.7)\n",
        "\n",
        "# F1 Score Comparison\n",
        "axes[0, 1].barh(results_df['Method'], results_df['F1'], color='lightgreen', edgecolor='black')\n",
        "axes[0, 1].set_xlabel('F1 Score', fontsize=12)\n",
        "axes[0, 1].set_title('F1 Score Comparison', fontsize=14, fontweight='bold')\n",
        "axes[0, 1].grid(True, alpha=0.3, axis='x')\n",
        "axes[0, 1].axvline(x=results_df['F1'].max(), color='red', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Precision-Recall Comparison\n",
        "axes[1, 0].scatter(results_df['Recall'], results_df['Precision'], s=200, alpha=0.6, edgecolors='black')\n",
        "for idx, row in results_df.iterrows():\n",
        "    axes[1, 0].annotate(row['Method'], (row['Recall'], row['Precision']), \n",
        "                       fontsize=8, ha='center')\n",
        "axes[1, 0].set_xlabel('Recall', fontsize=12)\n",
        "axes[1, 0].set_ylabel('Precision', fontsize=12)\n",
        "axes[1, 0].set_title('Precision-Recall Trade-off', fontsize=14, fontweight='bold')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Combined Metrics\n",
        "x_pos = np.arange(len(results_df))\n",
        "width = 0.25\n",
        "axes[1, 1].bar(x_pos - width, results_df['AUC'], width, label='AUC', alpha=0.8, edgecolor='black')\n",
        "axes[1, 1].bar(x_pos, results_df['F1'], width, label='F1', alpha=0.8, edgecolor='black')\n",
        "axes[1, 1].bar(x_pos + width, results_df['Precision'], width, label='Precision', alpha=0.8, edgecolor='black')\n",
        "axes[1, 1].set_xlabel('Method', fontsize=12)\n",
        "axes[1, 1].set_ylabel('Score', fontsize=12)\n",
        "axes[1, 1].set_title('Combined Metrics Comparison', fontsize=14, fontweight='bold')\n",
        "axes[1, 1].set_xticks(x_pos)\n",
        "axes[1, 1].set_xticklabels(results_df['Method'], rotation=45, ha='right', fontsize=8)\n",
        "axes[1, 1].legend(fontsize=10)\n",
        "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Select best method\n",
        "best_method = results_df.iloc[0]['Method']\n",
        "best_auc = results_df.iloc[0]['AUC']\n",
        "best_f1 = results_df.iloc[0]['F1']\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"BEST METHOD SELECTION\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Best method based on AUC: {best_method}\")\n",
        "print(f\"  AUC: {best_auc:.4f}\")\n",
        "print(f\"  F1 Score: {best_f1:.4f}\")\n",
        "print(f\"  Precision: {results_df.iloc[0]['Precision']:.4f}\")\n",
        "print(f\"  Recall: {results_df.iloc[0]['Recall']:.4f}\")\n",
        "\n",
        "# Train final model with best method\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"FINAL MODEL WITH BEST METHOD\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "X_best, y_best, use_class_weight = methods[best_method]\n",
        "\n",
        "if use_class_weight:\n",
        "    final_model = LogisticRegression(random_state=42, max_iter=1000, solver='lbfgs', \n",
        "                                     class_weight='balanced')\n",
        "else:\n",
        "    final_model = LogisticRegression(random_state=42, max_iter=1000, solver='lbfgs')\n",
        "\n",
        "final_model.fit(X_best, y_best)\n",
        "\n",
        "# Final predictions\n",
        "y_val_final_proba = final_model.predict_proba(X_val_resample)[:, 1]\n",
        "y_val_final_pred = final_model.predict(X_val_resample)\n",
        "\n",
        "# Final metrics\n",
        "final_auc = roc_auc_score(y_val, y_val_final_proba)\n",
        "final_f1 = f1_score(y_val, y_val_final_pred)\n",
        "final_precision = precision_score(y_val, y_val_final_pred)\n",
        "final_recall = recall_score(y_val, y_val_final_pred)\n",
        "\n",
        "print(f\"\\nFinal Model Performance:\")\n",
        "print(f\"  AUC: {final_auc:.4f}\")\n",
        "print(f\"  F1 Score: {final_f1:.4f}\")\n",
        "print(f\"  Precision: {final_precision:.4f}\")\n",
        "print(f\"  Recall: {final_recall:.4f}\")\n",
        "\n",
        "# Compare with original model (from cell 16)\n",
        "print(f\"\\nImprovement over original model:\")\n",
        "print(f\"  AUC improvement: {final_auc - val_auc:+.4f}\")\n",
        "print(f\"  F1 improvement: {final_f1 - val_f1:+.4f}\")\n",
        "print(f\"  Precision improvement: {final_precision - val_precision:+.4f}\")\n",
        "print(f\"  Recall improvement: {final_recall - val_recall:+.4f}\")\n",
        "\n",
        "# ROC Curve comparison\n",
        "fpr_final, tpr_final, _ = roc_curve(y_val, y_val_final_proba)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot(fpr_val, tpr_val, label=f'Original Model (AUC = {val_auc:.4f})', \n",
        "         linewidth=2, linestyle='--', alpha=0.7)\n",
        "plt.plot(fpr_final, tpr_final, label=f'Best: {best_method} (AUC = {final_auc:.4f})', \n",
        "         linewidth=2)\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random', alpha=0.5)\n",
        "plt.xlabel('False Positive Rate', fontsize=12)\n",
        "plt.ylabel('True Positive Rate', fontsize=12)\n",
        "plt.title('ROC Curve Comparison', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc='lower right', fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHkEHf0lx78v"
      },
      "source": [
        "_Optional, for those who have reached the end of the laptop_ 😊\n",
        "\n",
        "What was your impression of the work?\n",
        "What was difficult?\n",
        "What was interesting?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
